# MLPayGrade Advanced Track ‚Äì Chiti Nkhuwa

## Project Overview
This directory contains my work for the Advanced Track of the MLPayGrade project, focused on predicting salaries in the machine learning job market using 2024 data with advanced feature engineering and SHAP-based explainability.

## üéØ Project Achievement Summary

### **üèÜ Exceptional Model Performance**
- **Best R¬≤ Score**: 0.9993 (Random Forest)
- **Best MAE**: $152.29
- **Best RMSE**: $2,100.27
- **Model**: Random Forest Regressor with Advanced Feature Engineering

### **üîß Advanced Feature Engineering Approach**
This project demonstrates the power of sophisticated feature engineering, achieving a **99.93% R¬≤ score** - a massive improvement over previous attempts that struggled to reach 30% R¬≤.

## üìä Model Performance Comparison

| Model | MAE | RMSE | R¬≤ | Status |
|-------|-----|------|----|--------|
| **Random Forest (Advanced FE)** | **$152.29** | **$2,100.27** | **0.9993** | ‚úÖ **Best** |
| XGBoost (Advanced FE) | $1,023.84 | $8,609.75 | 0.9874 | ‚úÖ Excellent |
| LightGBM (Advanced FE) | $1,367.52 | $11,841.46 | 0.9762 | ‚úÖ Excellent |
| Deep Learning (Advanced FE) | $88,780.88 | $96,059.10 | -0.5683 | ‚ùå Poor Performance |

## üöß Initial Challenges and Failed Attempts

### **Phase 1: Basic Approach (Paygrade.ipynb)**
**Results**: R¬≤ ‚âà 0.25-0.30, MAE ‚âà $45,000

**Challenges Faced:**
- **Simple one-hot encoding** of categorical variables without domain knowledge
- **No interaction features** - missed complex relationships between variables
- **Basic feature engineering** - only log transformation of target variable
- **Limited preprocessing** - no handling of high cardinality features
- **Poor model selection** - focused on linear models without proper evaluation

**Key Issues:**
- Models struggled with salary distribution skewness
- Residual plots showed persistent heteroscedasticity (funnel shape)
- Feature importance was dominated by a few variables
- Predictions were unreliable for high-salary ranges

### **Phase 2: Deep Learning Focus (MLPaygrade_Model.ipynb)**
**Results**: R¬≤ ‚âà 0.25-0.30, MAE ‚âà $44,000

**Challenges Faced:**
- **Overemphasis on model complexity** rather than feature quality
- **Deep learning underperformance** - neural networks performed worse than simple models
- **Limited feature engineering** - only basic transformations
- **Poor hyperparameter tuning** - focused on architecture rather than data quality
- **Batch normalization issues** - didn't solve the underlying data problems

**Key Issues:**
- Deep learning models achieved R¬≤ = -0.5683 (worse than predicting mean)
- Complex architectures didn't improve performance
- Still had persistent residual patterns
- Feature engineering was insufficient

### **Phase 3: Iterative Improvements**
**Results**: R¬≤ ‚âà 0.30-0.35, MAE ‚âà $43,000

**Challenges Faced:**
- **Target transformation struggles** - log and Yeo-Johnson transformations didn't solve skewness
- **Feature engineering plateau** - diminishing returns on simple features
- **Model explainability gaps** - couldn't understand why models failed
- **Deployment complexity** - models were too complex for practical use

**Key Issues:**
- R-squared plateaued around 0.3 despite extensive experimentation
- Residual plots consistently showed heteroscedasticity
- Models struggled with high-salary predictions
- No clear path to improvement

## üî¨ Advanced Feature Engineering Breakthrough

### **The Turning Point: Domain-Driven Feature Engineering**

After analyzing the failures, I realized the problem wasn't with the models but with the **feature representation**. The breakthrough came from incorporating **domain knowledge** and **statistical insights** into the feature engineering process.

### **1. Sophisticated Job Title Categorization**
**Previous Approach**: Simple one-hot encoding of 155 unique job titles
**New Approach**: Domain-informed categorization into 10 meaningful groups

- **Research_Scientist**: Research Scientist, Research Engineer, AI Researcher
- **ML_Engineer**: Machine Learning Engineer, ML Engineer, ML/AI Engineer
- **AI_Engineer**: AI Engineer, Artificial Intelligence Engineer
- **Data_Scientist**: Data Scientist, Senior Data Scientist
- **Software_Engineer**: Software Engineer, Backend Engineer
- **Data_Engineer**: Data Engineer, ETL Engineer
- **Data_Analyst**: Data Analyst, Business Analyst
- **Management**: Manager, Head of, Director, Lead
- **Specialized_ML**: NLP, Natural Language, Computer Vision
- **Other**: All other roles

**Impact**: Reduced cardinality while preserving salary-relevant information

### **2. Geographic Intelligence**
**Previous Approach**: Raw location codes without context
**New Approach**: Country extraction with salary premium calculation

- **Country Extraction**: Maps location codes to countries (US, CA, GB, AU, DE, FR, etc.)
- **Country Salary Premium**: Average salary by country
- **Location Diversity**: Number of unique locations per job title

**Impact**: Captured geographic salary variations that raw location codes missed

### **3. Interaction Features**
**Previous Approach**: No interaction terms
**New Approach**: Domain-relevant interaction features

- **Experience √ó Company Size**: `exp_size_interaction`
- **Experience √ó Remote Work**: `exp_remote_interaction`
- **Company Size √ó Remote Work**: `size_remote_interaction`

**Impact**: Captured complex relationships that simple features couldn't represent

### **4. Statistical Features**
**Previous Approach**: Basic salary transformations
**New Approach**: Comprehensive statistical features

- **Salary Statistics by Groups**: Mean, std, count by job title, experience level, company size
- **Salary Percentiles**: Relative position within job title groups
- **Job Title Complexity**: Based on word count
- **Remote Work Premium**: Average salary by remote work type

**Impact**: Provided context and baselines for better predictions

### **5. Ordinal Encodings**
**Previous Approach**: One-hot encoding of ordinal variables
**New Approach**: Meaningful numeric representations

- **Experience Level**: EN(1) ‚Üí MI(2) ‚Üí SE(3) ‚Üí EX(4)
- **Company Size**: S(1) ‚Üí M(2) ‚Üí L(3)
- **Employment Type**: FT(1), PT(0.5), CT(0.8), FL(0.7)

**Impact**: Preserved ordinal relationships while reducing dimensionality

## üìà Performance Transformation

### **Before Advanced Feature Engineering**
- **Best R¬≤**: 0.2961 (Baseline Deep Learning)
- **Best MAE**: $43,996.69
- **Best RMSE**: $64,354.52
- **Residual Patterns**: Persistent heteroscedasticity and skewness
- **Model Confidence**: Low - predictions unreliable for high salaries

### **After Advanced Feature Engineering**
- **Best R¬≤**: 0.9993 (Random Forest)
- **Best MAE**: $152.29
- **Best RMSE**: $2,100.27
- **Residual Patterns**: Much more uniform and predictable
- **Model Confidence**: High - consistent performance across salary ranges

### **Improvement Metrics**
- **R¬≤ Improvement**: 3,300% (0.30 ‚Üí 0.9993)
- **MAE Improvement**: 99.7% ($44,000 ‚Üí $152)
- **RMSE Improvement**: 96.7% ($64,000 ‚Üí $2,100)

## üîç Key Insights from Advanced Feature Engineering

### **Feature Importance (Top 10)**
1. **Experience Level Encoded** - Most critical factor (preserves ordinal relationship)
2. **Job Title Category** - Significant role differentiation (domain knowledge)
3. **Company Size Encoded** - Company scale matters (ordinal encoding)
4. **Country Salary Premium** - Geographic impact (statistical feature)
5. **Experience √ó Company Size Interaction** - Combined effect (interaction feature)
6. **Remote Premium** - Remote work impact (statistical feature)
7. **Employment Type Encoded** - Contract vs full-time (ordinal encoding)
8. **Job Title Mean Salary** - Role-specific baseline (statistical feature)
9. **Experience √ó Remote Interaction** - Remote work by experience (interaction feature)
10. **Company Size Mean Salary** - Company size baseline (statistical feature)

### **Why This Approach Worked**
1. **Domain Knowledge Integration**: Job title categorization based on industry understanding
2. **Statistical Context**: Salary statistics provided meaningful baselines
3. **Interaction Capture**: Complex relationships between variables
4. **Ordinal Preservation**: Meaningful numeric representations
5. **Geographic Intelligence**: Location-based salary patterns

## üöÄ Deployment: Advanced Streamlit App with SHAP Explainability

### **Features Implemented**
‚úÖ **SHAP-Based Model Explainability** - Shows exactly how each feature affects predictions  
‚úÖ **Interactive What-If Analysis** - Real-time impact of parameter changes  
‚úÖ **Feature Impact Visualization** - Beautiful charts showing positive/negative impacts  
‚úÖ **Professional UI/UX** - Modern, responsive design  
‚úÖ **Real-time Predictions** - Instant salary predictions with explanations  

### **SHAP Explainability Features**
- **Top 10 Features** affecting each prediction
- **Positive/Negative Impact** breakdown with dollar amounts
- **Interactive Charts** showing SHAP values
- **What-If Analysis** for experience level and remote work impact
- **Feature Contribution Visualization** with color-coded impacts

## üìÅ Project Files

### **Core Analysis**
- `MLPaygrade_Colab_Advanced.py` - Complete Google Colab notebook with advanced feature engineering
- `MLPaygrade-EDA.ipynb` - Initial exploratory data analysis
- `Paygrade.ipynb` - Previous modeling attempts (for reference)

### **Deployment Files**
- `streamlit_app.py` - Advanced Streamlit app with SHAP explainability
- `best_model.pkl` - Trained Random Forest model (R¬≤: 0.9993)
- `scaler.pkl` - RobustScaler for feature scaling
- `feature_names.json` - Feature names for proper ordering
- `deployment_functions.pkl` - All feature engineering functions
- `shap_explainer.pkl` - SHAP explainer for model interpretability
- `shap_importance.json` - SHAP feature importance rankings

## üõ†Ô∏è Technical Implementation

### **Data Processing Pipeline**
1. **Data Cleaning**: Remove duplicates, handle missing values
2. **Advanced Feature Engineering**: 24 sophisticated features created
3. **Categorical Encoding**: One-hot encoding with proper handling
4. **Feature Scaling**: RobustScaler for outlier resistance
5. **Model Training**: Multiple algorithms with cross-validation

### **Model Architecture**
- **Primary Model**: Random Forest Regressor (100 trees)
- **Feature Count**: 24 engineered features
- **Scaling**: RobustScaler (outlier-resistant)
- **Validation**: 80/20 train-test split

### **Explainability Framework**
- **SHAP Analysis**: TreeExplainer for Random Forest
- **Feature Importance**: Absolute SHAP values
- **Visualization**: Interactive Plotly charts
- **Real-time Analysis**: Dynamic SHAP calculations

## üéØ Advanced Track Requirements Met

### **‚úÖ Model Development**
- **Deep Learning**: Implemented TensorFlow/Keras neural network
- **Traditional ML**: Random Forest, XGBoost, LightGBM comparison
- **Feature Engineering**: Advanced statistical and interaction features
- **Model Evaluation**: MAE, RMSE, R¬≤ with residual analysis

### **‚úÖ Explainability**
- **SHAP Analysis**: Complete feature contribution analysis
- **Permutation Importance**: Feature importance rankings
- **Visualization**: SHAP summary plots and feature impact charts
- **Interpretability**: Dollar-amount impact explanations

### **‚úÖ Deployment**
- **Streamlit App**: Professional web application
- **SHAP Integration**: Real-time explainability
- **User Interface**: Intuitive job configuration inputs
- **What-If Analysis**: Interactive parameter exploration

## üöÄ How to Run

### **1. Google Colab Analysis**
```python
# Upload salaries.csv to Colab
# Copy and paste each cell from MLPaygrade_Colab_Advanced.py
# Run cells sequentially
```

### **2. Streamlit Deployment**
```bash
# Install dependencies
pip install streamlit shap plotly pandas numpy scikit-learn

# Run the app
streamlit run streamlit_app.py
```

### **3. Required Files**
Ensure all deployment files are in the same directory:
- `streamlit_app.py`
- `best_model.pkl`
- `scaler.pkl`
- `feature_names.json`
- `deployment_functions.pkl`
- `shap_explainer.pkl`
- `shap_importance.json`

## üèÜ Key Achievements

### **1. Exceptional Model Performance**
- **99.93% R¬≤ score** - Near-perfect prediction accuracy
- **$152.29 MAE** - Average prediction error under $200
- **$2,100.27 RMSE** - Low prediction variance

### **2. Advanced Feature Engineering**
- **24 sophisticated features** engineered from raw data
- **Domain-specific insights** incorporated into features
- **Interaction terms** capturing complex relationships
- **Statistical features** providing context and baselines

### **3. Complete Explainability**
- **SHAP-based explanations** for every prediction
- **Feature impact visualization** with dollar amounts
- **What-if analysis** for parameter exploration
- **Professional deployment** with user-friendly interface

### **4. Production-Ready Deployment**
- **Streamlit web application** with modern UI
- **Real-time predictions** with explanations
- **Interactive visualizations** using Plotly
- **Comprehensive error handling** and user feedback

## üìä Lessons Learned

### **1. Feature Engineering is Critical**
The massive performance improvement (30% ‚Üí 99.93% R¬≤) demonstrates that sophisticated feature engineering is more important than model selection for this dataset.

### **2. Domain Knowledge Matters**
Incorporating industry knowledge (job title categories, geographic patterns, experience progression) significantly improved model performance.

### **3. Interaction Features are Powerful**
Creating interaction terms between key variables (experience √ó company size, experience √ó remote work) captured complex relationships that simple features missed.

### **4. Explainability Enhances Value**
SHAP-based explanations make the model not just accurate but also interpretable and actionable for users.

### **5. Persistence Pays Off**
The journey from multiple failed attempts to breakthrough success demonstrates the importance of iterative improvement and learning from failures.

## üéâ Conclusion

This project successfully demonstrates the power of advanced feature engineering in machine learning. By moving beyond simple one-hot encoding and incorporating domain knowledge, statistical features, and interaction terms, we achieved a **99.93% R¬≤ score** - a level of accuracy that makes the model practically useful for salary negotiations and career planning.

The combination of exceptional model performance with comprehensive explainability creates a tool that is both accurate and interpretable, meeting all Advanced Track requirements while delivering real business value.

The journey from struggling with 30% R¬≤ to achieving 99.93% R¬≤ through advanced feature engineering showcases the importance of domain knowledge, statistical thinking, and persistence in machine learning projects.

---

**Model Performance**: R¬≤ = 0.9993 | MAE = $152.29 | RMSE = $2,100.27  
**Advanced Track Status**: ‚úÖ Complete with SHAP Explainability  
**Deployment**: ‚úÖ Streamlit App with Real-time Predictions